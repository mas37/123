
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>train_ops &#8212; panna prerelease documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for train_ops</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<div class="viewcode-block" id="train_NN"><a class="viewcode-back" href="../neuralnet/train_ops.html#train_ops.train_NN">[docs]</a><span class="k">def</span> <span class="nf">train_NN</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">atomic_sequence</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train NN model, optimization step.</span>

<span class="sd">    Create an optimizer and apply to all trainable variables. Add moving</span>
<span class="sd">    average for all trainable variables.</span>

<span class="sd">    Args:</span>
<span class="sd">      loss: quantity to minimize</span>
<span class="sd">      global_step: Integer Variable counting the number of training steps</span>
<span class="sd">                   processed.</span>
<span class="sd">      lr : learning rate</span>
<span class="sd">      atomic_sequence: just for now here to simplify creation of histogram...</span>

<span class="sd">    Returns:</span>
<span class="sd">      train_op: op for training.</span>

<span class="sd">    TODO: refactor this routine.... it is too big and does too many stuff</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">apply_gradient_op</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gradient_application&#39;</span><span class="p">)</span>
    <span class="c1"># side note:</span>
    <span class="c1"># compute gradient and apply gradients together form what we called</span>
    <span class="c1"># the minimize function. minimize just does both the operations together</span>
    <span class="c1"># for more info: AdamOptimizer inherit form Optimizer. Here you can find</span>
    <span class="c1"># all those methods</span>

    <span class="n">dep</span> <span class="o">=</span> <span class="p">[</span><span class="n">apply_gradient_op</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">dep</span><span class="p">):</span>
        <span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_op</span></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">panna</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/simulation.html">Simulation - the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/inputs.html">Input of the network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/checkpoint.html">Checkpoint: saving and restart routine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/parser_callable.html">Input parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/networks.html">Network architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/regularizations.html">Regularizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/train_ops.html">Layers of the network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/layers.html">Real Layers of the network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neuralnet/variable_helpers.html">TF Variable helpers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Release notes &amp; To-do</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, pannadevs.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>