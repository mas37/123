%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{panna Documentation}
\date{Nov 15, 2018}
\release{prerelease}
\author{pannadevs}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\maketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


Automatically created by Sphinx using the
documentation in the source code


\chapter{Simulation - the data}
\label{\detokenize{neuralnet/simulation:simulation-the-data}}\label{\detokenize{neuralnet/simulation::doc}}\phantomsection\label{\detokenize{neuralnet/simulation:module-simulation}}\index{simulation (module)@\spxentry{simulation}\spxextra{module}}\index{Example (class in simulation)@\spxentry{Example}\spxextra{class in simulation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/simulation:simulation.Example}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{simulation.}}\sphinxbfcode{\sphinxupquote{Example}}}{\emph{g\_vectors}, \emph{species\_vector}, \emph{true\_energy}, \emph{zeros=None}, \emph{atomic\_species=None}, \emph{name=None}}{}
Example class

\end{fulllineitems}

\index{iterator\_over\_tfdata() (in module simulation)@\spxentry{iterator\_over\_tfdata()}\spxextra{in module simulation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/simulation:simulation.iterator_over_tfdata}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{simulation.}}\sphinxbfcode{\sphinxupquote{iterator\_over\_tfdata}}}{\emph{g\_size}, \emph{*args}, \emph{**kwargs}}{}
TFdata unpacker
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{g\_size}} \textendash{} size of the G’s

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{args}} \textendash{} all the tfdata files that one wants to parse

\end{itemize}

\end{description}\end{quote}
\begin{description}
\item[{kwargs:}] \leavevmode
zeros: list of zeros, one per species

\item[{Retrun:}] \leavevmode
iterator over the record in the files

\end{description}

\end{fulllineitems}



\chapter{Input of the network}
\label{\detokenize{neuralnet/inputs:module-inputs}}\label{\detokenize{neuralnet/inputs:input-of-the-network}}\label{\detokenize{neuralnet/inputs::doc}}\index{inputs (module)@\spxentry{inputs}\spxextra{module}}
Utilities to handling the input system
\index{parse\_fn\_v1() (in module inputs)@\spxentry{parse\_fn\_v1()}\spxextra{in module inputs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/inputs:inputs.parse_fn_v1}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{inputs.}}\sphinxbfcode{\sphinxupquote{parse\_fn\_v1}}}{\emph{example}, \emph{g\_size}, \emph{zeros}, \emph{n\_species}}{}
Parse TFExample records and perform simple data augmentation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{example}} \textendash{} a batch of example obj

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{g\_size}} \textendash{} size of the g\_vector

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{zeros}} \textendash{} array of zero’s one value per specie.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_species}} \textendash{} number of species

\end{itemize}

\item[{Returns}] \leavevmode
Sparse Tensor, (n\_atoms) value in range(n\_species)
g\_vectors\_tensor: Sparse Tensor, (n\_atoms, g\_size)
energy: true energy value corrected with the zeros

\item[{Return type}] \leavevmode
species\_tensor

\end{description}\end{quote}

\end{fulllineitems}

\index{input\_iterator() (in module inputs)@\spxentry{input\_iterator()}\spxextra{in module inputs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/inputs:inputs.input_iterator}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{inputs.}}\sphinxbfcode{\sphinxupquote{input\_iterator}}}{\emph{data\_dir}, \emph{batch\_size}, \emph{parse\_fn}, \emph{name}, \emph{shuffle\_buffer\_size\_multiplier=10}, \emph{prefetch\_buffer\_size\_multiplier=20}, \emph{num\_parallel\_readers=8}, \emph{num\_parallel\_calls=8}, \emph{*args}, \emph{oneshot=None}}{}
Construct input iterator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_dir}} \textendash{} directory for data, must contain a
“train\_tf subfolder”

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} \textendash{} batch size

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{parse\_fn}} \textendash{} function to parse the data from tfrecord file

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{name}} \textendash{} name scope

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{*\_buffer\_size\_multiplier}} \textendash{} batchsize times this number

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_parallel\_readers}} \textendash{} process that are doing Input form drive

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_parallel\_calls}} \textendash{} call of the parse function

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{oneshot}} \textendash{} experimental, do not set

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{TODO}} \textendash{} construct a double system to handle in\_place
evaluation of accuracy

\end{itemize}

\item[{Returns}] \leavevmode
initializable\_iterator, recover input data to feed the model

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}\begin{itemize}
\item {} 
shuffling batch and buffer size multiplier default are
randomly chosen by me

\item {} 
initializable iterator can be changed to one shot iterator
in future version to better comply with documentation

\item {} 
a maximum number of epoch should also be added to this routine.

\end{itemize}
\end{sphinxadmonition}

\end{fulllineitems}



\chapter{Checkpoint: saving and restart routine}
\label{\detokenize{neuralnet/checkpoint:module-checkpoint}}\label{\detokenize{neuralnet/checkpoint:checkpoint-saving-and-restart-routine}}\label{\detokenize{neuralnet/checkpoint::doc}}\index{checkpoint (module)@\spxentry{checkpoint}\spxextra{module}}\index{Checkpoint (class in checkpoint)@\spxentry{Checkpoint}\spxextra{class in checkpoint}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/checkpoint:checkpoint.Checkpoint}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{checkpoint.}}\sphinxbfcode{\sphinxupquote{Checkpoint}}}{\emph{filename}, \emph{atoms\_list=None}}{}
class to handle a Checkpoint

\end{fulllineitems}

\index{Parameters (class in checkpoint)@\spxentry{Parameters}\spxextra{class in checkpoint}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/checkpoint:checkpoint.Parameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{checkpoint.}}\sphinxbfcode{\sphinxupquote{Parameters}}}{\emph{file\_name}, \emph{atoms\_list=None}}{}
A class that load parameters form files generated by  and is compatible
with the Network object

\end{fulllineitems}



\chapter{Input parser}
\label{\detokenize{neuralnet/parser_callable:module-parser_callable}}\label{\detokenize{neuralnet/parser_callable:input-parser}}\label{\detokenize{neuralnet/parser_callable::doc}}\index{parser\_callable (module)@\spxentry{parser\_callable}\spxextra{module}}\index{get\_network\_architecture() (in module parser\_callable)@\spxentry{get\_network\_architecture()}\spxextra{in module parser\_callable}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/parser_callable:parser_callable.get_network_architecture}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{parser\_callable.}}\sphinxbfcode{\sphinxupquote{get\_network\_architecture}}}{\emph{value}}{}
parse the architecture format
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{value}} \textendash{} string like

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{layer\_size}} \textendash{} layer2\_size…

\end{itemize}

\item[{Returns}] \leavevmode
list of size per layer

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_network\_trainable() (in module parser\_callable)@\spxentry{get\_network\_trainable()}\spxextra{in module parser\_callable}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/parser_callable:parser_callable.get_network_trainable}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{parser\_callable.}}\sphinxbfcode{\sphinxupquote{get\_network\_trainable}}}{\emph{value}}{}
parse trainable list
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{value}} \textendash{} string like

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{1}} \textendash{} 0:1

\end{itemize}

\item[{Returns}] \leavevmode
list of trainable flag per layer

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Network architecture}
\label{\detokenize{neuralnet/networks:module-networks}}\label{\detokenize{neuralnet/networks:network-architecture}}\label{\detokenize{neuralnet/networks::doc}}\index{networks (module)@\spxentry{networks}\spxextra{module}}\index{network\_A2A() (in module networks)@\spxentry{network\_A2A()}\spxextra{in module networks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/networks:networks.network_A2A}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{networks.}}\sphinxbfcode{\sphinxupquote{network\_A2A}}}{\emph{batch\_of\_species}, \emph{batch\_of\_gvects}, \emph{layer\_size}, \emph{trainability}, \emph{gvect\_size}, \emph{batch\_size}, \emph{Nspecies}, \emph{atomic\_label}, \emph{import\_layer=None}, \emph{reuse=None}}{}~\begin{description}
\item[{New network with variable architecture annd}] \leavevmode
species resolved weights.

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_of\_species}} \textendash{} {[}batch size x max number of atoms per molecule{]}
species vector for each element of the batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_of\_gvects}} \textendash{} batch of gvectors

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{layer\_size}} \textendash{} a list of lists with the size of each
hidden layer, for each species

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{trainability}} \textendash{} a list of lists with the boolean flag
of the trainable status of each
hidden layer, for each species

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{gvect\_size}} \textendash{} gvector size

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} \textendash{} number of calculations in a batch

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{Nspecies}} \textendash{} number of species

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{import\_layer}} \textendash{} a list of lists of tuple,
each tuple has 2 elements,
weights and biases
that can be either a tensor with
correct shape or None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reuse}} \textendash{} whether to reuse variables with the same name

\end{itemize}

\item[{Returns}] \leavevmode

tf.tensor of energies
natoms\_batch: tf.tensor of number of atoms for each element of the
\begin{quote}

batch
\end{quote}


\item[{Return type}] \leavevmode
Energy

\end{description}\end{quote}

\end{fulllineitems}

\index{loss\_NN() (in module networks)@\spxentry{loss\_NN()}\spxextra{in module networks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/networks:networks.loss_NN}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{networks.}}\sphinxbfcode{\sphinxupquote{loss\_NN}}}{\emph{batch\_energies}, \emph{batch\_energies\_dft}, \emph{batch\_natoms}}{}
this is simply our cost function
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{= prediction of the network}} (\sphinxstyleliteralemphasis{\sphinxupquote{batch\_energies}}) \textendash{} 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{= energies labels}} (\sphinxstyleliteralemphasis{\sphinxupquote{batch\_energies\_dft}}) \textendash{} 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{= number of atoms}} (\sphinxstyleliteralemphasis{\sphinxupquote{batch\_natoms}}) \textendash{} 

\end{itemize}

\item[{Returns}] \leavevmode
the loss value
tensor with delta\_e for each element of the batch

\end{description}\end{quote}

\end{fulllineitems}

\index{eval\_network\_A2A (class in networks)@\spxentry{eval\_network\_A2A}\spxextra{class in networks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/networks:networks.eval_network_A2A}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{networks.}}\sphinxbfcode{\sphinxupquote{eval\_network\_A2A}}}{\emph{checkpoint}}{}
A2A network implementation

\end{fulllineitems}



\chapter{Regularizations}
\label{\detokenize{neuralnet/regularizations:module-regularizations}}\label{\detokenize{neuralnet/regularizations:regularizations}}\label{\detokenize{neuralnet/regularizations::doc}}\index{regularizations (module)@\spxentry{regularizations}\spxextra{module}}\index{l1l2\_regularizations() (in module regularizations)@\spxentry{l1l2\_regularizations()}\spxextra{in module regularizations}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/regularizations:regularizations.l1l2_regularizations}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{regularizations.}}\sphinxbfcode{\sphinxupquote{l1l2\_regularizations}}}{\emph{wscale\_l1}, \emph{wscale\_l2}, \emph{bscale\_l1}, \emph{bscale\_l2}}{}
Apply required regularizator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{w}} \textendash{} weights, b : biases

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{l1}} \textendash{} norm one prefactor if zero nothing gets applyed

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{l2}} \textendash{} norm two prefactor if zero nothing gets applyed

\end{itemize}

\item[{Returns}] \leavevmode
A scalar representing the overall regularization penalty for W
A scalar representing the overall regularization penalty for B

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Layers of the network}
\label{\detokenize{neuralnet/train_ops:module-train_ops}}\label{\detokenize{neuralnet/train_ops:layers-of-the-network}}\label{\detokenize{neuralnet/train_ops::doc}}\index{train\_ops (module)@\spxentry{train\_ops}\spxextra{module}}\index{train\_NN() (in module train\_ops)@\spxentry{train\_NN()}\spxextra{in module train\_ops}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/train_ops:train_ops.train_NN}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{train\_ops.}}\sphinxbfcode{\sphinxupquote{train\_NN}}}{\emph{loss}, \emph{global\_step}, \emph{lr}, \emph{atomic\_sequence}}{}
Train NN model, optimization step.

Create an optimizer and apply to all trainable variables. Add moving
average for all trainable variables.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{loss}} \textendash{} quantity to minimize

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{global\_step}} \textendash{} Integer Variable counting the number of training steps
processed.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lr}} \textendash{} learning rate

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{atomic\_sequence}} \textendash{} just for now here to simplify creation of histogram…

\end{itemize}

\item[{Returns}] \leavevmode
op for training.

\item[{Return type}] \leavevmode
train\_op

\end{description}\end{quote}

TODO: refactor this routine…. it is too big and does too many stuff

\end{fulllineitems}



\chapter{Real Layers of the network}
\label{\detokenize{neuralnet/layers:module-layers}}\label{\detokenize{neuralnet/layers:real-layers-of-the-network}}\label{\detokenize{neuralnet/layers::doc}}\index{layers (module)@\spxentry{layers}\spxextra{module}}\index{hidden\_layer\_gauss() (in module layers)@\spxentry{hidden\_layer\_gauss()}\spxextra{in module layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/layers:layers.hidden_layer_gauss}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{layers.}}\sphinxbfcode{\sphinxupquote{hidden\_layer\_gauss}}}{\emph{in\_tensor}, \emph{in\_size}, \emph{out\_size}, \emph{trainable}, \emph{init\_values=(None}, \emph{None)}}{}~\begin{description}
\item[{Define an all to all connected layer with species division and}] \leavevmode
Gaussian activation function.

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{in\_tensor}} \textendash{} input to be computed,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{in\_size}} \textendash{} last dimension of the input,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_size}} \textendash{} last dimension of the output,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{trainable}} \textendash{} whether we should train these weights

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{init\_values}} \textendash{} numpy arrays to initialize the tensors, weights and biases
None = default initialization

\end{itemize}

\item[{Returns}] \leavevmode
Output of the layer

\end{description}\end{quote}

weights variable will be named “weights”
bias variable will be named “bias”

\end{fulllineitems}

\index{hidden\_layer\_linear() (in module layers)@\spxentry{hidden\_layer\_linear()}\spxextra{in module layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neuralnet/layers:layers.hidden_layer_linear}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{layers.}}\sphinxbfcode{\sphinxupquote{hidden\_layer\_linear}}}{\emph{in\_tensor}, \emph{in\_size}, \emph{out\_size}, \emph{trainable}, \emph{init\_values=(None}, \emph{None)}}{}~\begin{description}
\item[{Define an all to all connected layer with species division and}] \leavevmode
linear activation function.
TODO: Make a single layer that accepts activation function

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{in\_tensor}} \textendash{} input to be computed,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{in\_size}} \textendash{} last dimension of the input,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_size}} \textendash{} last dimension of the output,

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{trainable}} \textendash{} whether we should train these weights

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{init\_values}} \textendash{} numpy arrays to initialize the tensors, weights and biases
None = default initialization

\end{itemize}

\item[{Returns}] \leavevmode
Output of the layer

\end{description}\end{quote}

weights variable will be named “weights”
bias variable will be named “bias”

\end{fulllineitems}



\chapter{TF Variable helpers}
\label{\detokenize{neuralnet/variable_helpers:tf-variable-helpers}}\label{\detokenize{neuralnet/variable_helpers::doc}}
These are specifically for TF variables
.. automodule:: variable\_helpers
.. autofunction:: \_variable\_on\_cpu
.. autofunction:: \_variable\_random\_uniform


\chapter{Release notes \& To-do}
\label{\detokenize{todo:release-notes-to-do}}\label{\detokenize{todo::doc}}
Here can be release notes change log and to do

\begin{sphinxadmonition}{note}{\label{todo:index-0}Todo:}
Update the below link when we add new guides on these.
\end{sphinxadmonition}
\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{c}
\item\relax\sphinxstyleindexentry{checkpoint}\sphinxstyleindexpageref{neuralnet/checkpoint:\detokenize{module-checkpoint}}
\indexspace
\bigletter{i}
\item\relax\sphinxstyleindexentry{inputs}\sphinxstyleindexpageref{neuralnet/inputs:\detokenize{module-inputs}}
\indexspace
\bigletter{l}
\item\relax\sphinxstyleindexentry{layers}\sphinxstyleindexpageref{neuralnet/layers:\detokenize{module-layers}}
\indexspace
\bigletter{n}
\item\relax\sphinxstyleindexentry{networks}\sphinxstyleindexpageref{neuralnet/networks:\detokenize{module-networks}}
\indexspace
\bigletter{p}
\item\relax\sphinxstyleindexentry{parser\_callable}\sphinxstyleindexpageref{neuralnet/parser_callable:\detokenize{module-parser_callable}}
\indexspace
\bigletter{r}
\item\relax\sphinxstyleindexentry{regularizations}\sphinxstyleindexpageref{neuralnet/regularizations:\detokenize{module-regularizations}}
\indexspace
\bigletter{s}
\item\relax\sphinxstyleindexentry{simulation}\sphinxstyleindexpageref{neuralnet/simulation:\detokenize{module-simulation}}
\indexspace
\bigletter{t}
\item\relax\sphinxstyleindexentry{train\_ops}\sphinxstyleindexpageref{neuralnet/train_ops:\detokenize{module-train_ops}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}